{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import IntegerLookup\n",
    "from tensorflow.keras.layers import Normalization\n",
    "from tensorflow.keras.layers import StringLookup\n",
    "import hypertune\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Preparing the data\n",
    "\n",
    "Let's download the data and load it into a Pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "def download_dataset(file_url=\"http://storage.googleapis.com/download.tensorflow.org/data/heart.csv\"):\n",
    "\n",
    "    dataframe = pd.read_csv(file_url)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "def split_data(dataframe):\n",
    "    val_dataframe = dataframe.sample(frac=0.2, random_state=1337)\n",
    "    train_dataframe = dataframe.drop(val_dataframe.index)\n",
    "\n",
    "    print(\n",
    "        \"Using %d samples for training and %d for validation\"\n",
    "        % (len(train_dataframe), len(val_dataframe))\n",
    "    )\n",
    "    return val_dataframe,train_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def dataframe_to_dataset(dataframe):\n",
    "    dataframe = dataframe.copy()\n",
    "    labels = dataframe.pop(\"target\")\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "    return ds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def encode_numerical_feature(feature, name, dataset):\n",
    "    # Create a Normalization layer for our feature\n",
    "    normalizer = Normalization()\n",
    "\n",
    "    # Prepare a Dataset that only yields our feature\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
    "\n",
    "    # Learn the statistics of the data\n",
    "    normalizer.adapt(feature_ds)\n",
    "\n",
    "    # Normalize the input feature\n",
    "    encoded_feature = normalizer(feature)\n",
    "    return encoded_feature\n",
    "\n",
    "\n",
    "def encode_categorical_feature(feature, name, dataset, is_string):\n",
    "    lookup_class = StringLookup if is_string else IntegerLookup\n",
    "    # Create a lookup layer which will turn strings into integer indices\n",
    "    lookup = lookup_class(output_mode=\"binary\")\n",
    "\n",
    "    # Prepare a Dataset that only yields our feature\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
    "\n",
    "    # Learn the set of possible string values and assign them a fixed integer index\n",
    "    lookup.adapt(feature_ds)\n",
    "\n",
    "    # Turn the string input into integer indices\n",
    "    encoded_feature = lookup(feature)\n",
    "    return encoded_feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Categorical features encoded as integers\n",
    "def get_features(train_ds):\n",
    "    sex = keras.Input(shape=(1,), name=\"sex\", dtype=\"int64\")\n",
    "    cp = keras.Input(shape=(1,), name=\"cp\", dtype=\"int64\")\n",
    "    fbs = keras.Input(shape=(1,), name=\"fbs\", dtype=\"int64\")\n",
    "    restecg = keras.Input(shape=(1,), name=\"restecg\", dtype=\"int64\")\n",
    "    exang = keras.Input(shape=(1,), name=\"exang\", dtype=\"int64\")\n",
    "    ca = keras.Input(shape=(1,), name=\"ca\", dtype=\"int64\")\n",
    "\n",
    "    # Categorical feature encoded as string\n",
    "    thal = keras.Input(shape=(1,), name=\"thal\", dtype=\"string\")\n",
    "\n",
    "    # Numerical features\n",
    "    age = keras.Input(shape=(1,), name=\"age\")\n",
    "    trestbps = keras.Input(shape=(1,), name=\"trestbps\")\n",
    "    chol = keras.Input(shape=(1,), name=\"chol\")\n",
    "    thalach = keras.Input(shape=(1,), name=\"thalach\")\n",
    "    oldpeak = keras.Input(shape=(1,), name=\"oldpeak\")\n",
    "    slope = keras.Input(shape=(1,), name=\"slope\")\n",
    "\n",
    "    all_inputs = [\n",
    "        sex,\n",
    "        cp,\n",
    "        fbs,\n",
    "        restecg,\n",
    "        exang,\n",
    "        ca,\n",
    "        thal,\n",
    "        age,\n",
    "        trestbps,\n",
    "        chol,\n",
    "        thalach,\n",
    "        oldpeak,\n",
    "        slope,\n",
    "    ]\n",
    "\n",
    "    # Integer categorical features\n",
    "    sex_encoded = encode_categorical_feature(sex, \"sex\", train_ds, False)\n",
    "    cp_encoded = encode_categorical_feature(cp, \"cp\", train_ds, False)\n",
    "    fbs_encoded = encode_categorical_feature(fbs, \"fbs\", train_ds, False)\n",
    "    restecg_encoded = encode_categorical_feature(restecg, \"restecg\", train_ds, False)\n",
    "    exang_encoded = encode_categorical_feature(exang, \"exang\", train_ds, False)\n",
    "    ca_encoded = encode_categorical_feature(ca, \"ca\", train_ds, False)\n",
    "\n",
    "    # String categorical features\n",
    "    thal_encoded = encode_categorical_feature(thal, \"thal\", train_ds, True)\n",
    "\n",
    "    # Numerical features\n",
    "    age_encoded = encode_numerical_feature(age, \"age\", train_ds)\n",
    "    trestbps_encoded = encode_numerical_feature(trestbps, \"trestbps\", train_ds)\n",
    "    chol_encoded = encode_numerical_feature(chol, \"chol\", train_ds)\n",
    "    thalach_encoded = encode_numerical_feature(thalach, \"thalach\", train_ds)\n",
    "    oldpeak_encoded = encode_numerical_feature(oldpeak, \"oldpeak\", train_ds)\n",
    "    slope_encoded = encode_numerical_feature(slope, \"slope\", train_ds)\n",
    "\n",
    "    all_features = layers.concatenate(\n",
    "        [\n",
    "            sex_encoded,\n",
    "            cp_encoded,\n",
    "            fbs_encoded,\n",
    "            restecg_encoded,\n",
    "            exang_encoded,\n",
    "            slope_encoded,\n",
    "            ca_encoded,\n",
    "            thal_encoded,\n",
    "            age_encoded,\n",
    "            trestbps_encoded,\n",
    "            chol_encoded,\n",
    "            thalach_encoded,\n",
    "            oldpeak_encoded,\n",
    "        ]\n",
    "    )\n",
    "    return all_features,all_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(all_features,all_inputs,num_neurons=32, learning_rate=0.1, dropout=0.5):\n",
    "    \n",
    "    x = layers.Dense(num_neurons, activation=\"relu\")(all_features)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    output = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    #tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logging/\")\n",
    "\n",
    "    model = keras.Model(all_inputs, output)\n",
    "    model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--learning_rate LEARNING_RATE]\n",
      "                             [--num_neurons NUM_NEURONS]\n",
      "                             [--num_epochs NUM_EPOCHS] [--dropout DROPOUT]\n",
      "                             [--working_dir WORKING_DIR]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/jupyter/.local/share/jupyter/runtime/kernel-e740d9e3-c997-45ca-ad55-c864aca5a4cb.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "def get_args():\n",
    "  '''Parses args. Must include all hyperparameters you want to tune.'''\n",
    "\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument(\n",
    "      '--learning_rate',\n",
    "      required=False,\n",
    "      type=float,\n",
    "      help='learning rate',\n",
    "      default=0.1)\n",
    "  parser.add_argument(\n",
    "      '--num_neurons',\n",
    "      required=False,\n",
    "      type=int,\n",
    "      default=64,\n",
    "      help='number of units in last hidden layer')\n",
    "  parser.add_argument(\n",
    "      '--num_epochs',\n",
    "      required=False,\n",
    "      type=int,\n",
    "      default=50,\n",
    "      help='number of units in last hidden layer')\n",
    "  parser.add_argument(\n",
    "      '--dropout',\n",
    "      required=False,\n",
    "      type=float,\n",
    "      default=0.5,\n",
    "      help='number of units in last hidden layer')\n",
    "  parser.add_argument(\n",
    "      '--working_dir',\n",
    "      required=False,\n",
    "      default=\"gs://mlops-vertex-jsk/output/\",\n",
    "      help='number of units in last hidden layer')  \n",
    "  args = parser.parse_args()\n",
    "  return args\n",
    "#new_model=redesign_model(64,0.1,0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    \"hidden_units\": 64,\n",
    "    \"epochs\":50,\n",
    "    \"learning_rate\":0.1,\n",
    "    \"dropout\":0.1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    args = get_args()\n",
    "\n",
    "    LOG_DIR='gs://mlops-vertex-jsk/hearts/training/logging'\n",
    "    dataframe=download_dataset(\"http://storage.googleapis.com/download.tensorflow.org/data/heart.csv\")\n",
    "    \n",
    "    train_dataframe, val_dataframe = split_data(dataframe)\n",
    "    \n",
    "\n",
    "    train_ds = dataframe_to_dataset(train_dataframe)\n",
    "    val_ds = dataframe_to_dataset(val_dataframe)\n",
    "    \n",
    "    train_ds = train_ds.batch(32)\n",
    "    val_ds = val_ds.batch(32)\n",
    "    \n",
    "    features,inputs=get_features(train_ds)\n",
    "    \n",
    "    model = create_model(features,inputs,args.num_neurons, args.learning_rate, args.dropout)\n",
    "    \n",
    "    \n",
    "    #tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=LOG_DIR)\n",
    "    history = model.fit(train_ds, epochs=args.num_epochs, validation_data=val_ds)\n",
    "    \n",
    "    # DEFINE METRIC\n",
    "    hp_metric = history.history['val_accuracy'][-1]\n",
    "\n",
    "    model.save(args.working_dir)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--learning_rate LEARNING_RATE]\n",
      "                             [--num_neurons NUM_NEURONS]\n",
      "                             [--num_epochs NUM_EPOCHS] [--dropout DROPOUT]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/jupyter/.local/share/jupyter/runtime/kernel-e740d9e3-c997-45ca-ad55-c864aca5a4cb.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "structured_data_classification_from_scratch",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "tf2-gpu.2-6.m80",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m80"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
